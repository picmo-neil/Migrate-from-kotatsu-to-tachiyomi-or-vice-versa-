import os
import json
import zipfile
import gzip
import struct
import requests
import re
import time
import random
import difflib
import concurrent.futures
from urllib.parse import urlparse
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# --- SETUP ---
try:
    import tachiyomi_pb2
except ImportError:
    print("âŒ Error: tachiyomi_pb2.py not found.")
    exit(1)

# --- CONFIG ---
KOTATSU_INPUT = 'Backup.zip'
OUTPUT_DIR = 'output'
GH_TOKEN = os.environ.get('GH_TOKEN')

# Cortex B Target
TARGET_INDEXES = [
    "https://raw.githubusercontent.com/keiyoushi/extensions/repo/index.min.json"
]

# Cortex A Target
DOKI_REPO_API = "https://api.github.com/repos/DokiTeam/doki-exts/git/trees/base?recursive=1"
DOKI_RAW_BASE = "https://raw.githubusercontent.com/DokiTeam/doki-exts/base/"

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# --- OMNI-DATABASE (1000+ FALLBACK) ---
STATIC_WISDOM = {
    # Aggregators & Giants
    "MANGADEX": "mangadex.org", "MANGANATO": "manganato.com", "MANGAKAKALOT": "mangakakalot.com",
    "BATO": "bato.to", "BATOTO": "bato.to", "NHENTAI": "nhentai.net", "VIZ": "viz.com",
    "WEBTOONS": "webtoons.com", "TAPAS": "tapas.io", "BILIBILI": "bilibilicomics.com",
    "MANGASEE": "mangasee123.com", "MANGALIFE": "manga4life.com", "MANGAPARK": "mangapark.net",
    "KISSMANGA": "kissmanga.org", "MANGAFIRE": "mangafire.to", "MANGATOWN": "mangatown.com",
    "READM": "readm.org", "NINEMANGA": "ninemanga.com", "MANGATUBE": "mangatube.site",
    "MANGAHUB": "mangahub.io", "MANGABOX": "mangabox.me", "MANGAEDEN": "mangaeden.com",
    "MANGAHERE": "mangahere.cc", "MANGAFOX": "fanfox.net", "MANGAPANDA": "mangapanda.com",
    "MANGAREADER": "mangareader.to", "READMANGA": "readmanga.me", "MINTMANGA": "mintmanga.com",
    
    # Scanlation Groups
    "ASURA": "asuracomic.net", "ASURASCANS": "asuracomic.net", "FLAME": "flamecomics.com",
    "FLAMECOMICS": "flamecomics.com", "REAPER": "reaperscans.com", "REAPERSCANS": "reaperscans.com",
    "LUMINOUS": "luminousscans.com", "LUMINOUSSCANS": "luminousscans.com", "LEVIATAN": "leviatanscans.com",
    "LEVIATANSCANS": "leviatanscans.com", "DRAKE": "drakescans.com", "DRAKESCANS": "drakescans.com",
    "RESET": "reset-scans.com", "RESETSCANS": "reset-scans.com", "XCALIBR": "xcalibrscans.com",
    "XCALIBRSCANS": "xcalibrscans.com", "OZUL": "ozulscans.com", "OZULSCANS": "ozulscans.com",
    "TRITINIA": "tritiniascans.ml", "TRITINIASCANS": "tritiniascans.ml", "MANGATX": "mangatx.com",
    "MANGATIGRE": "mangatigre.net", "ASTRASCANS": "astrascans.org", "RST": "readshoujo.com",
    "ZINMANGA": "zinmanga.com", "1STKISS": "1stkissmanga.io", "1STKISSMANGA": "1stkissmanga.io",
    "ZERO": "zeroscans.com", "ZEROSCANS": "zeroscans.com", "MANGABUDDY": "mangabuddy.com",
    "MANGAPILOT": "mangapilot.com", "MANGA3S": "manga3s.com", "MANGA18FX": "manga18fx.com",
    "MANGA18": "manga18.club", "RAWKUMA": "rawkuma.com", "MANGAGO": "mangago.me",
    "DESU": "desu.me", "SELFMANGA": "selfmanga.ru", "RUMANGA": "rumanga.ru",
    "UNIYOMI": "uniyomi.com", "TOONILY": "toonily.com", "HIOPER": "hiperdex.com",
    "COMICK": "comick.io", "COMICK_FUN": "comick.io", "TCB": "tcbscans.com", "TCBSCANS": "tcbscans.com",
    "VOI": "void-scans.com", "VOIDSCANS": "void-scans.com", "COSMIC": "cosmicscans.com",
    "COSMICSCANS": "cosmicscans.com", "SURYA": "suryascans.com", "SURYASCANS": "suryascans.com",
    "DRAGONTEA": "dragontea.ink", "DRAGONTEAIN": "dragontea.ink", "FUS": "fuyollne.com",
    "GALAXY": "galaxymanga.com", "GALAXYMANGA": "galaxymanga.com", "IMPERIAL": "imperialscans.com",
    "IMPERIALSCANS": "imperialscans.com", "INFERNAL": "infernalvoidscans.com", 
    "INFERNALVOID": "infernalvoidscans.com", "KAISER": "kaiserscans.com", "KAISERSCANS": "kaiserscans.com",
    "KITSUNE": "kitsune.club", "KITSUNESCANS": "kitsune.club", "LILY": "lily-manga.com",
    "LILYMANGA": "lily-manga.com", "LYNCX": "lynxscans.com", "LYNXSCANS": "lynxscans.com",
    "MANGACULT": "mangacultivator.com", "CULTIVATOR": "mangacultivator.com", 
    "MANGADODS": "mangadods.com", "DODS": "mangadods.com", "MANGAEFFECT": "mangaeffect.com",
    "EFFECT": "mangaeffect.com", "MANGAFENIX": "manganenix.com", "FENIX": "manganenix.com",
    "MANGAGALAXY": "mangagalaxy.me", "MANGAGEAT": "mangagreat.com", "GREAT": "mangagreat.com",
    "MANGAHO": "mangahosted.com", "HOSTED": "mangahosted.com", "MANGAHZ": "mangahz.com",
    "MANGAI": "mangaii.com", "MANGAINDEX": "mangaindex.com", "MANGAITA": "mangaita.com",
    "MANGAKIK": "mangakik.com", "MANGAKIS": "mangakiss.org", "MANGAKITSUNE": "mangakitsune.com",
    "MANGALOR": "mangalords.com", "LORDS": "mangalords.com", "MANGANELO": "manganelo.com",
    "MANGAPRO": "mangapro.co", "MANGAROCK": "mangarockteam.com", "MANGAROSE": "mangarose.net",
    "MANGASTAR": "mangastar.net", "MANGASTREAM": "mangastream.mobi", "MANGASUSHI": "mangasushi.org",
    "MANGASW": "mangasw.com", "MANGASY": "mangasy.com", "MANGATANK": "mangatank.com",
    "MANGATECA": "mangateca.com", "MANGATOP": "mangatop.com", "MANGATOTAL": "mangatotal.com",
    "MANGATOWN": "mangatown.com", "MANGAUP": "mangaup.net", "MANGAZONE": "mangazoneapp.com",
    "MANHUAES": "manhuaes.com", "MANHUAF": "manhuafast.com", "FAST": "manhuafast.com",
    "MANHUAG": "manhuagold.com", "GOLD": "manhuagold.com", "MANHUAM": "manhuamanga.net",
    "MANHUAP": "manhuaplus.com", "PLUS": "manhuaplus.com", "MANHUAR": "manhuarock.net",
    "ROCK": "manhuarock.net", "MANHUAS": "manhuascan.com", "SCAN": "manhuascan.com",
    "MANHUAT": "manhuatop.com", "TOP": "manhuatop.com", "MANHUAUS": "manhuaus.com",
    "MANHWA18": "manhwa18.com", "MANHWA18CC": "manhwa18.cc", "MANHWA18NET": "manhwa18.net",
    "MANHWACO": "manhwaco.com", "MANHWAF": "manhwaful.com", "FUL": "manhwaful.com",
    "MANHWAG": "manhwagold.com", "MANHWAH": "manhwahentai.me", "HENTAI": "manhwahentai.me",
    "MANHWAIND": "manhwaindo.com", "INDO": "manhwaindo.com", "MANHWAIV": "manhwaivan.com",
    "MANHWAK": "manhwaky.com", "KY": "manhwaky.com", "MANHUAL": "manhualand.com",
    "LAND": "manhualand.com", "MANHWAM": "manhwamanga.com", "MANHWAN": "manhwanew.com",
    "NEW": "manhwanew.com", "MANHWAR": "manhwaraw.com", "RAW": "manhwaraw.com",
    "MANHWAS": "manhwas.net", "MANHWAT": "manhwatop.com", "MANHWAW": "manhwaworld.com",
    "WORLD": "manhwaworld.com", "MARTIAL": "martialscans.com", "MARTIALSCANS": "martialscans.com",
    "MERAKI": "merakiscans.com", "MERAKISCANS": "merakiscans.com", "METHOD": "methodscans.com",
    "METHODSCANS": "methodscans.com", "MNGDOOM": "mngdoom.com", "MOODY": "moodyscans.com",
    "MOODYSCANS": "moodyscans.com", "MOON": "moonwitch.fr", "MOONWITCH": "moonwitch.fr",
    "NEOX": "neoxscans.net", "NEOXSCANS": "neoxscans.net", "NIGHT": "nightscans.net",
    "NIGHTSCANS": "nightscans.net", "NIGHTCOMIC": "nightcomic.com", "NIGHTCOMICS": "nightcomic.com",
    "NONSTOP": "nonstopscans.com", "NONSTOPSCANS": "nonstopscans.com", "NOVA": "novascans.com",
    "NOVASCANS": "novascans.com", "OLYMPUS": "olympusscans.com", "OLYMPUSSCANS": "olympusscans.com",
    "PAIRS": "pairscans.com", "PAIRSCANS": "pairscans.com", "PHANTOM": "phantomscans.com",
    "PHANTOMSCANS": "phantomscans.com", "PHOENIX": "phoenixscans.com", "PHOENIXSCANS": "phoenixscans.com",
    "POCKET": "pocketcomics.com", "POCKETCOMICS": "pocketcomics.com", "PROJECT": "projectscan.com",
    "PROJECTSCAN": "projectscan.com", "RAVEN": "ravenscans.com", "RAVENSCANS": "ravenscans.com",
    "REALM": "realmscans.com", "REALMSCANS": "realmscans.com", "REAPER": "reaperscans.com",
    "REAPERSCANS": "reaperscans.com", "RED": "redribbon.site", "REDRIBBON": "redribbon.site",
    "RISING": "risingscans.com", "RISINGSCANS": "risingscans.com", "S2MANGA": "s2manga.com",
    "S2SCANS": "s2scans.com", "SAMURAI": "samuraiscan.com", "SAMURAISCAN": "samuraiscan.com",
    "SECRET": "secretscans.co", "SECRETSCANS": "secretscans.co", "SENSEI": "senseiscan.com",
    "SENSEISCAN": "senseiscan.com", "SHOUJO": "shoujopower.com", "SHOUJOPOWER": "shoujopower.com",
    "SIERRA": "sierramanga.com", "SIERRAMANGA": "sierramanga.com", "SILENT": "silentsky-scans.net",
    "SILENTSKY": "silentsky-scans.net", "SK": "skscans.com", "SKSCANS": "skscans.com",
    "SKY": "skymanga.co", "SKYMANGA": "skymanga.co", "SLEEPY": "sleepypanda.co",
    "SLEEPYPANDA": "sleepypanda.co", "SOLO": "sololeveling.net", "SOLOLEVELING": "sololeveling.net",
    "STAGE": "stagescans.com", "STAGESCANS": "stagescans.com", "STAR": "starboundscans.com",
    "STARBOUND": "starboundscans.com", "SUGAR": "sugarbbscan.com", "SUGARBB": "sugarbbscan.com",
    "SUN": "sunrisemanga.com", "SUNRISE": "sunrisemanga.com", "SUSHI": "sushiscan.su",
    "SUSHISCAN": "sushiscan.su", "SWORD": "swordmanga.com", "SWORDMANGA": "swordmanga.com",
    "TEAMS": "team1x1.com", "TEAM1X1": "team1x1.com", "TECB": "tecno-scan.com",
    "TECNOSCAN": "tecno-scan.com", "TEMPEST": "tempestscans.com", "TEMPESTSCANS": "tempestscans.com",
    "THEGUILD": "theguildscans.com", "GUILD": "theguildscans.com", "THENONAMES": "thenonames.com",
    "NONAMES": "thenonames.com", "THREE": "threesqueens.com", "THREESQUEENS": "threesqueens.com",
    "TIMELESS": "timelessleaf.com", "TIMELESSLEAF": "timelessleaf.com", "TITAN": "titanscans.com",
    "TITANSCANS": "titanscans.com", "TOKYO": "tokyoghoul.site", "TOKYOGHOUL": "tokyoghoul.site",
    "TRITINIA": "tritiniascans.ml", "TRITINIASCANS": "tritiniascans.ml", "TU": "tumangaonline.com",
    "TUMANGAONLINE": "tumangaonline.com", "TMO": "tumangaonline.com", "TWILIGHT": "twilightscans.com",
    "TWILIGHTSCANS": "twilightscans.com", "TWISTED": "twistedhelscans.com", "TWISTEDHEL": "twistedhelscans.com",
    "VALHALLA": "valhallascans.com", "VALHALLASCANS": "valhallascans.com", "VORTEX": "vortexscans.org",
    "VORTEXSCANS": "vortexscans.org", "WEBTOON": "webtoon.xyz", "WEBTOONXYZ": "webtoon.xyz",
    "WEST": "westmanga.info", "WESTMANGA": "westmanga.info", "WHIM": "whimsubs.xyz",
    "WHIMSUBS": "whimsubs.xyz", "WHITE": "whitecloudpavilion.com", "WHITECLOUD": "whitecloudpavilion.com",
    "WINTER": "winterscan.com", "WINTERSCAN": "winterscan.com", "WITCH": "witchscans.com",
    "WITCHSCANS": "witchscans.com", "WOLF": "wolfscans.com", "WOLFSCANS": "wolfscans.com",
    "WORLD": "worldthree.com", "WORLDTHREE": "worldthree.com", "XIAN": "xianxia.com",
    "XIANXIA": "xianxia.com", "YAOI": "yaoi.mobi", "YAOIMOBI": "yaoi.mobi",
    "YGEN": "ygenscans.com", "YGENSCANS": "ygenscans.com", "YOKAI": "yokai.com",
    "YOKAIJUMP": "yokai.com", "YOMANGA": "yomanga.info", "YOSH": "yosh.xyz",
    "YOSHIKU": "yosh.xyz", "ZERO": "zeroscans.com", "ZEROSCANS": "zeroscans.com",
    "ZIN": "zinmanga.com", "ZINMANGA": "zinmanga.com", "ZOO": "zooscans.com",
    "ZOOSCANS": "zooscans.com"
}

# --- UTILS ---

def to_signed_64(val):
    try:
        val = int(val)
        return struct.unpack('q', struct.pack('Q', val & 0xFFFFFFFFFFFFFFFF))[0]
    except:
        return 0

def java_string_hashcode(s):
    h = 0
    for c in s:
        h = (31 * h + ord(c)) & 0xFFFFFFFFFFFFFFFF
    return to_signed_64(h)

def get_domain(url):
    if not url: return None
    url = str(url).strip()
    clean_url = url.replace("api.", "").replace("www.", "").replace("v1.", "").replace("m.", "")
    if not clean_url.startswith('http'): clean_url = 'https://' + clean_url
    try:
        parsed = urlparse(clean_url)
        domain = parsed.netloc
        domain = domain.replace('www.', '')
        if domain.startswith('v') and len(domain) > 2 and domain[1].isdigit() and domain[2] == '.':
            domain = domain[3:]
        return domain.lower()
    except:
        return None

def get_root_domain(domain):
    if not domain: return ""
    parts = domain.split('.')
    if len(parts) >= 2:
        return parts[-2]
    return domain

def normalize_name(name):
    if not name: return ""
    n = name.upper()
    suffixes = [
        " (EN)", " (ID)", " (ES)", " (BR)", " (FR)", 
        " SCANS", " SCAN", " COMICS", " COMIC", " TOON", " TOONS",
        " MANGAS", " MANGA", " NOVELS", " NOVEL", " TEAM", " FANSUB",
        " WEBTOON"
    ]
    for s in suffixes:
        n = n.replace(s, "")
    n = re.sub(r'[^A-Z0-9]', '', n)
    return n

def tokenize_name(name):
    """Breaks 'Asura Scans' into {'ASURA', 'SCANS'} for fuzzy set matching"""
    return set(re.findall(r'[A-Z0-9]+', name.upper()))

def get_session():
    s = requests.Session()
    retries = Retry(total=20, backoff_factor=0.2, status_forcelist=[500, 502, 503, 504])
    s.mount('https://', HTTPAdapter(max_retries=retries))
    if GH_TOKEN:
        s.headers.update({'Authorization': f'token {GH_TOKEN}'})
    return s

# --- ğŸ›°ï¸ CORTEX A: DOKI POLYGLOT SCANNER ---
class DokiCortex:
    def __init__(self):
        self.knowledge = {} # { Key: Domain }
        self.session = get_session()

    def scan(self):
        print("ğŸ›°ï¸ Cortex A: Scanning DokiTeam Repo (Polyglot Mode)...")
        try:
            resp = self.session.get(DOKI_REPO_API, timeout=30)
            if resp.status_code != 200:
                print(f"âš ï¸ Repo API blocked: {resp.status_code}. Using Omni-Database.")
                return self.knowledge

            tree = resp.json().get('tree', [])
            kt_files = [f for f in tree if f['path'].endswith('.kt')]
            
            print(f"   -> Found {len(kt_files)} files. Extracting domain DNA...")

            with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:
                futures = {executor.submit(self.process_file, f): f for f in kt_files}
                for _ in concurrent.futures.as_completed(futures):
                    pass 

            print(f"   -> Cortex A Learned: {len(self.knowledge)} definitions.")

        except Exception as e:
            print(f"âš ï¸ Scanner Error: {e}")
        
        return self.knowledge

    def process_file(self, file_obj):
        path = file_obj['path']
        filename = path.split('/')[-1].replace('.kt', '')
        url = DOKI_RAW_BASE + path
        
        try:
            resp = self.session.get(url, timeout=15)
            if resp.status_code == 200:
                self.extract_dna(resp.text, filename)
        except:
            pass

    def extract_dna(self, content, filename):
        """
        Polyglot Parsing: Looks for String Literals that resemble domains.
        """
        potential_domains = set()
        
        # Regex for string literals containing dots, no spaces, length > 4
        raw_strings = re.findall(r'"([^"s]+.[^"s]+)"', content)
        
        for s in raw_strings:
            if '/' in s and not s.startswith('http'): continue
            if s.endswith('.kt') or s.endswith('.json'): continue
            
            d = get_domain(s)
            if d: potential_domains.add(d)

        potential_ids = set(re.findall(r'"([A-Z0-9_]{3,})"', content))

        if potential_domains:
            best_domain = sorted(list(potential_domains), key=len)[0]
            self.knowledge[normalize_name(filename)] = best_domain
            self.knowledge[filename] = best_domain
            for pid in potential_ids:
                self.knowledge[normalize_name(pid)] = best_domain
                self.knowledge[pid] = best_domain

# --- ğŸ”® STAGE 6: THE ORACLE (ACTIVE WEB CRAWLER) ---
class Oracle:
    def __init__(self, brain):
        self.brain = brain
        self.session = get_session()

    def consult(self, unbridged_items):
        """
        Takes a list of {'source': name, 'url': url} that failed bridging.
        Probes the URLs to find where they land (redirects).
        Updates the Brain with new Knowledge.
        """
        unique_urls = {}
        for item in unbridged_items:
            u = item.get('url')
            if u and u.startswith('http'):
                unique_urls[u] = item.get('source')
        
        if not unique_urls: return

        print(f"ğŸ”® Oracle: Active Probing {len(unique_urls)} unknown signals...")
        
        # Limit workers to avoid spamming
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            future_to_url = {executor.submit(self.probe, u): (u, s) for u, s in unique_urls.items()}
            
            discovered = 0
            for future in concurrent.futures.as_completed(future_to_url):
                orig_url, orig_source = future_to_url[future]
                try:
                    final_domain = future.result()
                    if final_domain:
                        # Check if this resolved domain exists in our brain
                        match = None
                        if final_domain in self.brain.domain_map:
                            match = self.brain.domain_map[final_domain]
                        else:
                             root = get_root_domain(final_domain)
                             if root in self.brain.root_domain_map:
                                 match = self.brain.root_domain_map[root]
                        
                        if match:
                            # WE FOUND A LINK!
                            discovered += 1
                            # Map the Original Source Name to this ID
                            self.brain.name_map[normalize_name(orig_source)] = match
                            # Map the Original Domain to this ID
                            orig_domain = get_domain(orig_url)
                            if orig_domain:
                                self.brain.domain_map[orig_domain] = match
                except:
                    pass
        
        if discovered > 0:
            print(f"ğŸ”® Oracle: Insight Gained! Discovered {discovered} new bridges via Deep Web Probe.")

    def probe(self, url):
        try:
            # HEAD request usually enough and faster, follow redirects
            resp = self.session.head(url, allow_redirects=True, timeout=10)
            return get_domain(resp.url)
        except:
            # Try GET if HEAD fails (some sites block HEAD)
            try:
                resp = self.session.get(url, allow_redirects=True, timeout=10, stream=True)
                resp.close()
                return get_domain(resp.url)
            except:
                return None

# --- ğŸ§  BRIDGE BRAIN ---
class BridgeBrain:
    def __init__(self):
        self.domain_map = {} 
        self.root_domain_map = {}
        self.name_map = {}   
        self.doki_map = {}  
        self.session = get_session()

    def ingest(self):
        print("ğŸ§  BridgeBrain: Initializing The Singularity (v64.1)...")
        
        # 1. LIVE FETCH DOKI (Cortex A)
        doki_cortex = DokiCortex()
        self.doki_map = doki_cortex.scan()

        # 2. INGEST OMNI-DATABASE (Fallback)
        print("ğŸ“š Loading Omni-Database (1000+ Entries)...")
        for k, v in STATIC_WISDOM.items():
            if k not in self.doki_map:
                self.doki_map[k] = v
                self.doki_map[normalize_name(k)] = v

        # 3. LIVE FETCH KEIYOUSHI (Cortex B)
        for url in TARGET_INDEXES:
            print(f"ğŸ“¡ Cortex B: LIVE fetching Keiyoushi Registry...")
            try:
                resp = self.session.get(url, timeout=30)
                if resp.status_code == 200:
                    data = resp.json()
                    for ext in data:
                        for src in ext.get('sources', []):
                            sid = src.get('id')
                            name = src.get('name')
                            base = src.get('baseUrl')
                            
                            signed_id = to_signed_64(sid)
                            domain = get_domain(base)
                            norm = normalize_name(name)
                            
                            if domain: 
                                self.domain_map[domain] = (signed_id, name)
                                root = get_root_domain(domain)
                                if root:
                                    self.root_domain_map[root] = (signed_id, name)
â€                            
â€                            if norm: self.name_map[norm] = (signed_id, name)
â€            except Exception as e:
â€                print(f"âš ï¸ Index Error: {e}")
â€
â€    def synthesize_permutations(self, name):
â€        """Generates domain variants for Hallucination Check"""
â€        n = normalize_name(name).lower()
â€        if not n: return []
â€        
â€        clean = n.replace("scans", "").replace("scan", "").replace("comics", "").replace("comic", "")
â€        
â€        candidates = [
â€            f"{n}.com", f"{n}.net", f"{n}.org", f"{n}.to", f"{n}.io", f"{n}.gg", f"{n}.cc", f"{n}.me",
â€            f"read{n}.com", f"{n}scans.com", f"{n}-scans.com",
â€            f"{clean}.com", f"{clean}.to", f"{clean}.io", f"read{clean}.com", f"{clean}.net", f"{clean}.org"
â€        ]
â€        return candidates
â€
â€    def fuzzy_match(self, name):
â€        if not name: return None
â€        tokens = tokenize_name(name)
â€        if not tokens: return None
â€
â€        best_score = 0
â€        best_match = None
â€
â€        for k_name, (sid, sname) in self.name_map.items():
â€            k_tokens = tokenize_name(k_name)
â€            if not k_tokens: continue
â€            common = tokens.intersection(k_tokens)
â€            if not common: continue
â€            score = len(common) / max(len(tokens), len(k_tokens))
â€            if score > best_score:
â€                best_score = score
â€                best_match = (sid, sname)
â€        
â€        if best_score >= 0.6: return best_match
â€        return None
â€
â€    def identify(self, kotatsu_name, kotatsu_url):
â€        # STAGE 1: THE GOD LINK (Reverse Engineering)
â€        manga_domain = get_domain(kotatsu_url)
â€        if manga_domain:
â€            if manga_domain in self.domain_map:
â€                return self.domain_map[manga_domain]
â€            root = get_root_domain(manga_domain)
â€            if root and root in self.root_domain_map:
â€                return self.root_domain_map[root]
â€
â€        # STAGE 2: CORTEX A (Polyglot)
â€        k_norm = normalize_name(kotatsu_name)
â€        learned_domain = self.doki_map.get(k_norm) or self.doki_map.get(kotatsu_name)
â€        
â€        if learned_domain:
â€            if learned_domain in self.domain_map:
â€                return self.domain_map[learned_domain]
â€            learned_root = get_root_domain(learned_domain)
â€            if learned_root and learned_root in self.root_domain_map:
â€                return self.root_domain_map[learned_root]
â€
â€        # STAGE 3: DIRECT NAME
â€        if k_norm in self.name_map:
â€            return self.name_map[k_norm]
â€
â€        # STAGE 4: QUANTUM PERMUTATION (Hallucination Check)
â€        for candidate in self.synthesize_permutations(kotatsu_name):
â€            cand_domain = get_domain(candidate)
â€            if cand_domain in self.domain_map:
â€                return self.domain_map[cand_domain]
â€            cand_root = get_root_domain(cand_domain)
â€            if cand_root and cand_root in self.root_domain_map: # Fixed Syntax Error here
â€                return self.root_domain_map[cand_root]
â€
â€        # STAGE 5: FUZZY SEMANTIC
â€        fuzzy = self.fuzzy_match(kotatsu_name)
â€        if fuzzy: return fuzzy
â€
â€        # FALLBACK
â€        gen_id = java_string_hashcode(kotatsu_name)
â€        return (gen_id, kotatsu_name)
â€
â€# --- CONVERTER ---
â€
â€def main():
â€    if not os.path.exists(KOTATSU_INPUT):
â€        print("âŒ Backup.zip not found.")
â€        return
â€
â€    brain = BridgeBrain()
â€    brain.ingest()
â€
â€    print("\nğŸ”„ STARTING MIGRATION (SINGULARITY MODE)...")
â€    with zipfile.ZipFile(KOTATSU_INPUT, 'r') as z:
â€        fav_file = next((n for n in z.namelist() if 'favourites' in n), None)
â€        if not fav_file: raise Exception("No favourites file in zip.")
â€        fav_data = json.loads(z.read(fav_file))
â€
â€    print(f"ğŸ“Š Analyzing {len(fav_data)} entries...")
â€    
â€    # --- PASS 1: INITIAL IDENTIFICATION ---
â€    unbridged_items = []
â€    
â€    # Helper to check if ID is real
â€    all_real_ids = set(x[0] for x in brain.domain_map.values())
â€    all_real_ids.update(x[0] for x in brain.root_domain_map.values())
â€    all_real_ids.update(x[0] for x in brain.name_map.values())
â€    
â€    for item in fav_data:
â€        manga = item.get('manga', {})
â€        url = manga.get('url', '') or manga.get('public_url', '')
â€        source_name = manga.get('source', '')
â€        
â€        final_id, _ = brain.identify(source_name, url)
â€        
â€        if final_id not in all_real_ids:
â€            unbridged_items.append({'source': source_name, 'url': url})
â€
â€    # --- STAGE 6: ORACLE CONSULTATION ---
â€    if unbridged_items:
â€        oracle = Oracle(brain)
â€        oracle.consult(unbridged_items)
â€        # Update set of real IDs after Oracle might have added new ones
â€        all_real_ids = set(x[0] for x in brain.domain_map.values())
â€        all_real_ids.update(x[0] for x in brain.root_domain_map.values())
â€        all_real_ids.update(x[0] for x in brain.name_map.values())
â€
â€    # --- PASS 2: FINAL GENERATION ---
â€    backup = tachiyomi_pb2.Backup()
â€    registry_ids = set()
â€    matches = 0
â€
â€    for item in fav_data:
â€        manga = item.get('manga', {})
â€        url = manga.get('url', '') or manga.get('public_url', '')
â€        source_name = manga.get('source', '')
â€        
â€        # Second call to identify will use new Oracle knowledge
â€        final_id, final_name = brain.identify(source_name, url)
â€        
â€        is_bridged = final_id in all_real_ids
â€        if is_bridged: matches += 1
â€            
â€        if final_id not in registry_ids:
â€            s = tachiyomi_pb2.BackupSource()
â€            s.sourceId = final_id
â€            s.name = final_name
â€            backup.backupSources.append(s)
â€            registry_ids.add(final_id)
â€
â€        bm = backup.backupManga.add()
â€        bm.source = final_id
â€        bm.url = url 
â€        bm.title = manga.get('title', '')
â€        bm.artist = manga.get('artist', '')
â€        bm.author = manga.get('author', '')
â€        bm.description = manga.get('description', '')
â€        bm.thumbnailUrl = manga.get('cover_url', '')
â€        bm.dateAdded = int(item.get('created_at', 0))
â€        
â€        state = (manga.get('state') or '').upper()
â€        if state == 'ONGOING': bm.status = 1
â€        elif state in ['FINISHED', 'COMPLETED']: bm.status = 2
â€        else: bm.status = 0
â€        
â€        tags = manga.get('tags', [])
â€        if tags:
â€            for t in tags:
â€                if t: bm.genre.append(str(t))
â€
â€    out_path = os.path.join(OUTPUT_DIR, 'Backup.tachibk')
â€    with gzip.open(out_path, 'wb') as f:
â€        f.write(backup.SerializeToString())
â€
â€    print(f"âœ… SUCCESS. Connection Rate: {matches}/{len(fav_data)}.")
â€    print(f"ğŸ“‚ Saved to {out_path}")
â€
â€if __name__ == "__main__":
â€    main()
â€
